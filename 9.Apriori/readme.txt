查找有毒蘑菇的相关特征。可以用关联规则来解释。
我们首先关心毒蘑菇有哪些特征，因此找出所有支持度（毒蘑菇，特征x）大于给定值的特征。
然后对于这些特征，我们计算特征x→毒蘑菇的可信度=支持度（特征x,毒蘑菇）/支持度（特征）
最后给出一个列表和分析结果
最后可以看出(28, 85) 和(39, 59, 85) 这2个特征作为区分有毒和没毒的特征效果最好，可信度达到了97%和94%

思路整理：
首先导入数据。
然后找出单字母的频繁集，并且储存在一个字典中
然后建立一个超集形成器，输入子集元素，输出超集元素
然后建立一个循环器，通过输入数据集，频繁子集元素和字典，在字典中添加它的超集的频繁集。这样就得出所有频繁集
然后为了减少计算量，我们把数据分成2份，一份是毒蘑菇的，一份是非毒蘑菇的。对于毒蘑菇集，我们找出大于给定支持度的所有特征，然后对于这些特征，我们遍历非毒蘑菇数据集，找出这些特征的数据条数，然后综合给出特征在总数据集中的支持度。然后就可以算出特征到毒蘑菇的可信度
最后综合并给出结果

写代码时觉察到的决策树算法的缺点和改进点：
1.Apriori算法并没有探求买了多件同种商品的情况
2。Apriori原理其实是说如果一个集合是非频繁的，那么包含它的集合（超集）也是非频繁的
3.支持度表示数据集中包含某特征的所有记录所占的比例
4.可信度是基于关联规则定义的，尿布→葡萄酒的可信度是指支持度（尿布，葡萄酒）/支持度（尿布）
5.支持度一般小于0.5，可信服一般大于0.5。支持度等于0.5表示有一半的数据集有这个特征，可信度小于等于0.5这个特征一点也不可信
6.和书上的结果有出入，原因是书上的结果只列了一部分？

学到的python代码：
1.字典的包含？
2.frozenset是不可变集合，能够作为字典的键，而set类型的集合不能
3.在做二次递归的时候比如for i in data: for j in data:,这样做的效率是很低的。
还不如for i in range(shape(data)[0]): for j in range(i,shape(data)[0]) :。计算量貌似很减少一半
4.a|b是集合运算，求并
5，要得到frozenset({'86'})，不能直接frozenset（‘86’），要先转化为列表，再转化为集合：frozenset（[‘86’]）
6.word.difference(frozenset(line))==frozenset()判断集合line是否包含集合word
更简便的方法为word.issubset(frozenset(line))





